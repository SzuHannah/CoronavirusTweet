---
title: "data_processing"
author: "Sukyoung Cho, Hannah Wang"
date: "3/25/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(rtweet)
library(tidyr)
library(tidytext)
library(readr)
#for topic cluster
library(vsp)
library(tm)
library(stringr)
library(wordcloud2)
library(ggplot2)
#plot multiple plots altogether
library(gridExtra)
#for twitter
library(httr)
library(rtweet)
library(tidyverse)
library(lubridate)
```


## R Markdown

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:

```{r cars}
confirmed_coro_jhu<-read_csv("https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_global.csv")
confirmed_coro_jhu
```


```{r}
#covid<-readRDS("covid_19_tweets.rds")
#select the columns needed
#covidds<-covid%>%select(date, user_id, status_id,screen_name, text, flock_label, flock_category)
covidds<-readRDS("covidds.rds")
wordfreq_bydate_top50_wocoron<-readRDS("wordfreq_bydate_top50_wocoron.rds")
#7 levels in total for flock_category:"liberals""conservatives" "media""issue-centric""pop culture""academia""other", but actually there's no data for "other"        
#flock_categorylevel<-levels(covidds$flock_category)

##split the data frame into a list by date
#group by date, and then splic according to date key
covid_day<-covidds %>% group_by(date)
#check the groupkey
groupkeyday<-group_keys(covid_day)
#name the groupkey
groupkeyday_name<-groupkeyday %>% mutate(group_name=str_c(as.character(date),"-","all"))
#split by date
covid_dayls<-covidds %>% group_by(date)%>%group_split()%>%setNames(groupkeyday_name$group_name)

##split the data frame into a list by date and flock category
#group by date and flock_category, and then split according to this date+flock_category key
covid_dayflock<-covidds %>% group_by(date, flock_category)
#check the keys in covid_dayflock
groupkey<-group_keys(covid_dayflock)
#groupkey
#name the group keys, i.e. combine date and flock_category to create a group_name
groupkey_name<-groupkey%>%mutate(group_name=str_c(as.character(date),"-",as.character(flock_category)))
#groupkey_name
#can do it in one line, and also assign category name with groupkey_name$group_name
covid_dayflockls<-covidds %>% group_by(date, flock_category)%>%group_split()%>%setNames(groupkey_name$group_name)

##covidds by date with all categories and by date-flockcategories
covid_day_allbyflockls<-append(covid_dayls,covid_dayflockls)

##split the data frame into a list by flock
covid_flockls<-split(covidds,f=covidds$flock_category)[1:6]
covid_flockls<-readRDS("covid_flockls.rds")
```

###try cluster tweets within each flock across the 60 days (i.e.cluster all tweets within one flock into 30 topics, and each tweet has different time stamp)
```{r}
#use covidflottls
# covidflottls<-readRDS("covidflottls.rds")
#create cast_sparse for each flock, didn't use lapply, 'cause it'll cause session runout
dtliberal<-cast_sparse(covidflottls[["liberals"]],tweet,word)
dtconser<-cast_sparse(covidflottls[["conservatives"]],tweet,word)
dtmedia<-cast_sparse(covidflottls[["media"]],tweet,word)
dtissue<-cast_sparse(covidflottls[["issue-centric"]],tweet,word)
dtpop<-cast_sparse(covidflottls[["pop culture"]],tweet,word)
dtacademia<-cast_sparse(covidflottls[["academia"]],tweet,word)
dtliberal<-readRDS("dtliberal.rds")
dtconser<-readRDS("dtconservatives.rds")
dtmedia<-readRDS("dtmedia.rds")
dtissue<-readRDS("dtissue.rds")
dtpop<-readRDS("dtpop.rds")
dtacademia<-readRDS("dtacademia.rds")
faliberal<-falsflock[["liberals"]]
faconserv<-falsflock[["conservatives"]]
famedia<-falsflock[["media"]]
faissue<-falsflock[["issue-centric"]]
fapop<-falsflock[["pop culture"]]
faacademia<-falsflock[["academia"]]
#vsp for each flock
faliberal<-vsp(dtliberal,k=30)
#plot_varimax_z_pairs(faliberal, 1:10)
topTweets = 10
topDoclib<-faliberal$Z %>% apply(2, function(t) which(rank(-t, ties.method = "random") <= topTweets))

faconserv<-vsp(dtconser,k=30)
topDoccons<-faconserv$Z %>% apply(2, function(t) which(rank(-t, ties.method = "random") <= topTweets))

famedia<-vsp(dtmedia,k=30)
topDocmedia<-famedia$Z %>% apply(2, function(t) which(rank(-t, ties.method = "random") <= topTweets))

faissue<-vsp(dtissue,k=30)
topDocissue<-faissue$Z %>% apply(2, function(t) which(rank(-t, ties.method = "random") <= topTweets))

fapop<-vsp(dtpop,k=30)
topDocpop<-fapop$Z %>% apply(2, function(t) which(rank(-t, ties.method = "random") <= topTweets))

faacademia<-vsp(dtacademia, k=30)
topDocacademia<-faacademia$Z %>% apply(2, function(t) which(rank(-t, ties.method = "random") <= topTweets))
#make the fa into a list
falsflock<-list(faliberal,faconserv,famedia,faissue,fapop,faacademia)
#names(falsflock)<-c("liberals","conservatives","media","issue-centric","pop culture","academia")
lmformds<-readRDS("lmformds.rds")
logtotalconf<-lmformds%>%select(c(date,total_confirm))%>%mutate(log_conf=log(total_confirm))
#logtotalconf
covid_flockls<-readRDS("covid_flockls.rds")
#covidflock dataset combined with log total confirm number
covidflock_conf<-lapply(covid_flockls, function(x){
  x%>%left_join(logtotalconf,by="date")
})
saveRDS(covidflock_conf,"covidflock_conf.rds")
topDocflock<-list(topDoclib,topDoccons,topDocmedia,topDocissue,topDocpop,topDocacademia)
#names(topDocflock)<-c("liberals","conservatives","media","issue-centric","pop culture","academia")
#get the correspondent tweets
topiclsbyflo<-lapply(seq_along(topDocflock), function(i){
  df<-topDocflock[[i]]
  txtdf<-covidflock_conf[[i]]
  ls<-list()
  for(j in 1:ncol(df)){
    name<-paste("topic", j)
    ls[[j]]<-tibble(tweet = txtdf$text[df[,j]], status_id=txtdf$status_id[df[,j]],screen_name=txtdf$screen_name[df[,j]])
    names(ls)[j]<-name
  }
  return(ls)
})

#paste twitter link to each dataframe, use this one!!!
#get_embed url of twitter post
get_embed <- function(status_id){

  api_result <- httr::GET(paste0(
    "https://publish.twitter.com/oembed?url=https%3A%2F%2Ftwitter.com%2FInterior%2Fstatus%2F",
    status_id))

  api_content <- httr::content(api_result)
  html_content <- api_content[["html"]]
  
  return(html_content)
}

topiclsbyflowithlink<-lapply(topiclsbyflo,function(ls){
  lapply(ls, function(topic){
    topic%>%
      mutate(status_url=paste0("https://twitter.com/",screen_name,"/status/",status_id))%>%
      mutate(embed_url=map(status_id,get_embed))
  })
})

glmlib
covidflock_conf<-readRDS("covidflock_conf.rds")
falsflock<-readRDS("falsflock.rds")
#names(topiclsbyflo)<-c("liberals","conservatives","media","issue-centric","pop culture","academia")
#try linear model
ylib<-covidflock_conf[["liberals"]]$log_conf
libfaZ<-falsflock[["liberals"]]$Z
glmlib<-glm(ylib~as.matrix(libfaZ), family = "gaussian") %>% summary
#glm of log(totalconfirm)~each topic
glmls<-lapply(seq_along(covidflock_conf),function(x){
  y<-covidflock_conf[[x]]$log_conf
  z<-falsflock[[x]]$Z
  glm(y~as.matrix(z),family="gaussian")%>%summary
})
names(glmls)<-c("liberals","conservatives","media","issue-centric","pop culture","academia")
summary(glmls[["liberals"]])
```


##make clusters/topics for the whole covidds
```{r}
#all covid tweet token list, but will cause seesion abort..
# covidttls<-readRDS("covidttls.rds")
# covidflottls<-readRDS("covidflottls.rds")
# covid_dayflockls<-readRDS("covid_dayflockls.rds")
#covid_flockls<-readRDS("covid_flockls.rds")
#covidtext by flock
covidtextls <- lapply(covid_flockls,function(x){
   tibble(tweet = 1:nrow(x), text = x$text)
})

#this sampled df will not cause session abort
#covidtext by day-flock sampled, eventually use this one, since i make the df, and then sampled, the index can be used to search in original unsampled dataframe
coviddayflotextls<-readRDS("coviddayflotextls.rds")
coviddayflotextls <- lapply(covid_dayflockls,function(x){
   df<-tibble(tweet = 1:nrow(x), text = x$text)
   sample_frac(df,0.5)
})


# this does a lot of processing! 
#  to lower, remove @ # , . 
#  often these make sense on a first cut.
#  worth revisiting before "final results"!
covidflottls <- lapply(covidtextls, function(x){
  x %>% unnest_tokens(word, text)
})

#sampled day flockls
#coviddayflottls<-readRDS("coviddayflottls.rds")
coviddayflottls <- lapply(coviddayflotextls, function(x){
  x %>% unnest_tokens(word, text)
})

#sampled day-flock sparse matrix
#coviddayflodt<-readRDS("coviddayflodt.rds")
coviddayflodt<-lapply(coviddayflottls, function(x){
  dt<-cast_sparse(x,tweet,word)
})
#sampled day-flock factors
fadayflo<-lapply(coviddayflodt, function(x){
  fa<-vsp(x,k=30)
})

#fadayflo<-readRDS("fadayflo.rds")
#the index matrix for correspondent category
topTweets = 5
topDocdayflo<-lapply(fadayflo, function(x){
  topDoc<-x$Z %>% apply(2, function(t) which(rank(-t, ties.method = "random") <= topTweets))
})
#try plot the result from 2020-02-01-media
plot_varimax_z_pairs(fadayflo[[3]], 1:10)
y_0201case<-tidypredcovds%>%filter(date=="2020-02-01")%>%filter(case_type=="total_confirm")
y_0201case
#topDocdayflo<-readRDS("topDocdayflo.rds")
#covid_dayflockls<-readRDS("covid_dayflockls.rds")

#the list of the top tweets by topic
topicls<-lapply(seq_along(topDocdayflo), function(i){
  df<-topDocdayflo[[i]]
  txtdf<-covid_dayflockls[[i]]
  ls<-list()
  for(j in 1:ncol(df)){
    name<-paste("topic", j)
    ls[[j]]<-tibble(tweet = txtdf$text[df[,j]], status_id=txtdf$status_id[df[,j]],screen_name=txtdf$screen_name[df[,j]])
    names(ls)[j]<-name
  }
  return(ls)
})

##name the topicls
#name the topic list with date-category
catname<-names(topDocdayflo)
names(topicls)<-catname
#topicls<-readRDS("topicls.rds")

#paste twitter link to each dataframe, use this one!!!
#get_embed url of twitter post
get_embed <- function(status_id){

  api_result <- httr::GET(paste0(
    "https://publish.twitter.com/oembed?url=https%3A%2F%2Ftwitter.com%2FInterior%2Fstatus%2F",
    status_id))

  api_content <- httr::content(api_result)
  html_content <- api_content[["html"]]
  
  return(html_content)
}

topiclswithlink<-lapply(topicls,function(ls){
  lapply(ls, function(topic){
    topic%>%
      mutate(status_url=paste0("https://twitter.com/",screen_name,"/status/",status_id))%>%
      mutate(embed_url=map(status_id,get_embed))
  })
})

result<-httr::GET("https://publish.twitter.com/oembed?url=https%3A%2F%2Ftwitter.com%2FInterior%2Fstatus%2F507185938620219395")
httr::content(result)
#saveRDS(topiclswithlink,"topiclswithlink.rds")
```

###sentiment analysis
```{r}
#sentiment analysis
library(sentimentr)
coviddayflotextls
outls<-lapply(seq_along(coviddayflotextls),function(x){
  ds<-bind_rows(coviddayflotextls[x], .id = "column_label")
  with(ds,sentiment_by(
    get_sentences(text),
    list(column_label)))
})
bind_rows(coviddayflotextls[1], .id = "column_label")
uncombinout<-lapply(outls, function(x){
  uncombine(x)
})
saveRDS(uncombinout,"uncombinout.rds")

uncombineds<-bind_rows(uncombinout)
saveRDS(uncombineds,"uncombineds.rds")
#filter by month
filterm<-uncombineds%>%filter(str_detect(column_label,"2020-02-02"))
#filter by flock label
filterflo<-filterm%>%filter(str_detect(column_label,c("liberals","conservatives")))
#plot sentiment
ggplot(filterflo, aes(x=column_label, y=sentiment))+geom_boxplot()+theme_bw()
#str_detect from stringr
feb02<-outds%>%filter(str_detect(column_label,"2020-02-02"))
plot(feb02)
feb02%>%filter(str_detect(column_label,c("liberals","conservatives")))
#coviddayflotextds<-bind_rows(coviddayflotextls, .id="column_label")
out<-with(coviddayflotextds,
          sentiment_by(get_sentences(text),
                       list(column_label)))
#saveRDS(coviddayflotextsent,"coviddayflotextsend.rds")
coviddayflotextsent<-readRDS("coviddayflotextsend.rds")
plot(combine(coviddayflotextsent[[1]]))

```



###find common words across days
```{r}
wordfreq_bydateflo_top15_wocoron<-readRDS("wordfreq_bydateflo_top15_wocoron.rds")
  all60topwordls<-wordfreq_bydateflo_top15_wocoron[1:60]
summaryls<-lapply(seq_along(all60topwordls), function(x){
  df<-all60topwordls[[x]]%>%mutate(date=names(all60topwordls[x]))
})
summaryds<-bind_rows(summaryls,.id="column_label")
wordoccur40<-summaryds%>%group_by(word)%>%filter(n()>40)
#the words that occur in more than 40 days
thewords<-wordoccur40%>%group_by(word)%>%summarise(count=n())
wordpredictors<-thewords$word
saveRDS(wordpredictors,"wordpredictors.rds")
dswithwordpred<-wordoccur40%>%filter(word %in% wordpredictors)%>%mutate(date=as.Date(date))
dswithwuhan<-dswithwordpred%>%filter(word=="wuhan")
dswithwordpred
#each word is a data frame: count v.s. date
wordpredls<-dswithwordpred%>%group_by(word)%>%group_split()
names(wordpredls)<-wordpredictors
#plot each word's count v.s. date(take log for word, since pandemic increases exponentially in the end of march) 
dswithwordpred %>% ggplot()+geom_line(aes(x = as.Date(date), y = log(n), color=word))
#or maybe do facetchart?
```

###coronavirus data
```{r}
covidspread<-read_csv("full_data.csv")
covidus_casedeath<-covidspread %>% filter(location == "United States")%>%filter(between(date,as.Date("2020-02-01"),as.Date("2020-03-31")))
covidus_casedeathsel<-covidus_casedeath%>%select(date, new_cases, new_deaths,total_cases,total_deaths)
covidtestconfir<-read_csv("covid-19-total-confirmed-cases-vs-total-tests-conducted.csv")
covidtestconfir%>%filter(Entity=="United States")
covidustestconf<-covidtestconfir%>%filter(Entity == "United States")%>%mutate(Date=mdy(Date))%>%filter(between(Date, as.Date("2020-02-01"),as.Date("2020-03-31")))
#add a column that calculate confim/test
covidusspread<-covidustestconf%>%mutate(date = Date)
covidusspreadsel<-covidusspread%>%select(date,`Total confirmed cases of COVID-19 (cases)`,`Total tests`)%>%
  mutate(total_confirm=`Total confirmed cases of COVID-19 (cases)`,total_test=`Total tests`)%>%mutate(confirm_test=(total_confirm/total_test)*100)

covidusspreadsel%>%ggplot()+geom_line(aes(x = date, y = confirm_test),color="red")+
                        geom_line(aes(x = date, y = total_confirm),color="blue")+
                        geom_line(aes(x = date, y = total_test),color="black")
```


###combine coronavirus and tweet
```{r}
covidusspreadsel<-covidusspreadsel%>%select(date, total_confirm,total_test,confirm_test)
covidusspreadsel
covidus_casedeathsel<-covidus_casedeathsel%>%select(-total_cases)
wordpredcov<-lapply(wordpredls, function(x){
  x %>% left_join(covidusspreadsel)%>%left_join(covidus_casedeathsel)
})

wordpredcovds<-bind_rows(wordpredcov)
#tidy form for ggpplot
tidypredcovds<-wordpredcovds%>%pivot_longer(cols = c("total_confirm","total_test","confirm_test","new_cases","new_deaths","total_deaths"),names_to = "case_type",values_to = "case_number")%>%drop_na()
epidcov<-wordpredcov[[4]]
tidyepidcov<-epidcov%>%pivot_longer(cols = c("total_confirm","total_test","confirm_test","new_cases","new_deaths","total_deaths"),names_to = "case_type",values_to = "case_number")%>%drop_na()
#take log for case_number since they are all increasing exponentially
tidyepidcov%>%ggplot(aes(x=n, y=log(case_number)))+geom_point()+facet_wrap(~case_type,scales = "free")

#untidy form for linear model
lmformds<-wordpredcovds%>%group_by(date)%>%pivot_wider(names_from = word,values_from = n)
selection=c("epidemic", "infected")
type=c("total_confirm")
lms<-lmformds[,c(selection,type)]
lms[,type]<-log(lms[,type])
lms
lmformds
loglmds<-lmformds%>%mutate(total_confirm=log(total_confirm),total_test=log(total_test),confirm_test=log(confirm_test),
                  new_cases=log(new_cases),new_deaths=log(new_deaths),total_deaths=log(total_deaths))
#tryout
totalconfword<-wordpredcovds%>%select(word,n,date,total_confirm)%>%group_by(date)%>%pivot_wider(names_from = word,values_from = n)
totalconfword_wona<-totalconfword%>%drop_na()
totalconfword_wona
#plot(totalconfword_wona$realdonaldtrump,totalconfword_wona$total_confirm)
lm1<-lm(log(total_confirm)~.-date,totalconfword_wona)
summary(lm1)
##save RDS to use
saveRDS(tidypredcovds,"tidypredcovds.rds")
saveRDS(lmformds,"lmformds.rds")
saveRDS(loglmds,"loglmds.rds")
#prevent overfitting, just take 3 words as predictors
```



###practice vsp
```{r}
# make the document-term matrix.    
#   I sometimes call this the bag-of-words-matrix.
dtliberal<-cast_sparse(coviddayflottls[[1]], tweet, word)
faliberal = vsp(dtliberal, k=30)
plot_varimax_z_pairs(faliberal, 1:10)

dtmedia<-cast_sparse(coviddayflottls[[3]], tweet, word)
famedia = vsp(dtmedia, k=30)
plot_varimax_z_pairs(famedia, 1:10)
textmedia<-covid_dayflockls[[2]]

textmedia
topDoc[,1]
topiclsmedia<-list()
  for(j in 1:ncol(topDoc)){
  name<-paste("topic", j)
  topiclsmedia[[j]]<-tibble(tweet = textmedia$text[topDoc[,j]], status_id=textmedia$status_id[topDoc[,j]],screen_name=textmedia$screen_name[topDoc[,j]])
  names(topiclsmedia)[j]<-name
}
#  then see what the document clusters appear to be:
# 
topTweets = 3
# just run the next code chunk...

topDoc = faliberal$Z %>% 
  apply(2,
        function(x) which(rank(-x, ties.method = "random") <= topTweets)
  )
covid_dayflockls<-readRDS("covid_dayflockls.rds")
textliberal<-covid_dayflockls[[1]]

for(j in 1:ncol(topDoc)){
  paste("topic", j, "\n \n") %>% cat
  textliberal$text[topDoc[,j]] %>% print
  paste("\n \n \n") %>% cat
}
```

##rtweet
```{r}
##check tweet with status id:
#https://twitter.com/screen_name/status/status_id
```


```{r} 
#karl's example
dt = cast_sparse(tt, tweet, word)
dt[1:5,1:10]
str(dt)
dim(dt)
hist(rowSums(dt))
cs = colSums(dt)
hist(log(cs[cs>1]))
#use library(vsp)
fa = vsp(dt, k = 30)
fa
#if you see radial streaks, aligned with the axes in this plot:
plot_varimax_z_pairs(fa, 1:10)

#  then see what the document clusters appear to be:
# 
topTweets = 3
# just run the next code chunk...

topDoc = fa$Z %>% 
  apply(2,
        function(x) which(rank(-x, ties.method = "random") <= topTweets)
  )

topDoc

for(j in 1:ncol(topDoc)){
  paste("topic", j, "\n \n") %>% cat
  text_df$text[topDoc[,j]] %>% print
  paste("\n \n \n") %>% cat
}
library(glmnet)
glm(y~as.matrix(fa$Z), family = "binomial") %>% summary
cvfit = cv.glmnet(as.matrix(fa$Z),y, 
                  family = "binomial",
                  type.measure = "class", 
                  nfolds = 5, 
                  alpha = .1)
plot(cvfit)
```

  ##stopwords
```{r}
#tidytext builtin stop words
data("stop_words")
#my stop words
mystopword<-tibble(word = c("https","t.co","ー"))
#rtwwet built-in stop words
twitterstop<-stopwordslangs%>%filter(lang == "en")
```

## summarize all data's word frequency
```{r}
#the focused dataset
#covidds
#retrieve only the text column
text_covid <- tibble(tweet = 1:nrow(covidds), text = covidds$text)
text_covid

#unnest the words into tokens
token_txtcovid  = text_covid %>% unnest_tokens(word, text)
#str(token_txtcovid)
#column "tweet" means the row of the tweet in the text_covid, same number means they came from the same tweet

#exclude the stop words
tidy_txtcovid<-token_txtcovid %>% anti_join(stop_words) %>% anti_join(mystopword) %>% anti_join(twitterstop)

#summarize the word frequency
txtcovid_wordfreq<-tidy_txtcovid %>% count(word, sort = TRUE)
txtcovid_wordfreq
```

##do summarize for each date frame in the list(an element of the list is a day-category table)
```{r}
#retreive text column for each date
text_bydateflo_covid<-lapply(covid_day_allbyflockls, function(x){
  tibble(tweet = 1:nrow(x), text = x$text)
})

wordfreq_bydateflo_covid<-lapply(text_bydateflo_covid, function(x){
  x %>% unnest_tokens(word, text)%>%
    anti_join(stop_words)%>%
    anti_join(mystopword)%>%
    anti_join(twitterstop)%>%
    count(word, sort = TRUE)
})

#exclude the word coronavirus, and include 20 words
wordfreq_bydateflo_top20_wocoron<-lapply(wordfreq_bydateflo_covid, function(x){
  x %>% 
    filter(!word %in% c("coronavirus", "covid19", "covid", "coronavid19", "corona", "covid2019"))%>%
    top_n(20)
})

#exclude the word coronavirus, and include 15 words
wordfreq_bydateflo_top15_wocoron<-lapply(wordfreq_bydateflo_covid, function(x){
  x %>% 
    filter(!word %in% c("coronavirus", "covid19", "covid", "coronavid19", "corona", "covid2019"))%>%
    top_n(15)
})

#exclude the word coronavirus, and include 50 words
wordfreq_bydateflo_top50_wocoron<-lapply(wordfreq_bydateflo_covid, function(x){
  x %>% 
    filter(!word %in% c("coronavirus", "covid19", "covid", "coronavid19", "corona", "covid2019"))%>%
    top_n(50)
})
```

##try playing around with list
```{r}

##names of the elements in the list
#names(wordfreq_bydateflo_top20_wocoron) 

##try filter the list
#flockname<-list("2020-02-01-liberals"=,"2020-02-01-conservatives")
#flockofinterest<-wordfreq_bydateflo_top20_wocoron[unlist(flockname)]

##try filter the list with flock name and date
chosendate<-"2020-02-01"
flockchoice<-list("liberals"="liberals","conservatives" = "conservatives")
#filter list elements of interest
flockname<-flockchoice
indexls<-lapply(flockname, function(x){
  str_c(as.character(chosendate),"-",x)
})
index<-unlist(indexls)
interestls<-wordfreq_bydateflo_top20_wocoron[index]
interestls[[1]]
##plot all dataframe in the chosen list
plot.list = lapply(interestls, function(x) {
  df = x
  ggplot(df, aes(word)) +
    theme_bw() +
    geom_bar(aes(weight = n))
})

plot.list = lapply(seq_along(interestls), function(i) {
  labname = names(interestls)[i]
  df = interestls[[i]]
  ggplot(df, aes(word)) +
    theme_bw() +
    geom_bar(aes(weight = n))+
    labs(title = labname)
})
# Lay out all the plots together
#library(gridExtra)
#can plot on multiple pages: marrageGrob
marrangeGrob(plot.list,nrow=1,ncol=2)
do.call(grid.arrange, plot.list) 
#customized plot arrange
n <- length(plot.list)
nRow <- floor(sqrt(n))
do.call("grid.arrange", c(plot.list, nrow=nRow))
```



##do summarize for each data frame in the list(an element of the list is a day)
```{r}
#retreive text column for each date
text_bydate_covid<-lapply(covidls, function(x){
  tibble(tweet = 1:nrow(x), text = x$text)
})

wordfreq_bydate_covid<-lapply(text_bydate_covid, function(x){
  x %>% unnest_tokens(word, text)%>%
    anti_join(stop_words)%>%
    anti_join(mystopword)%>%
    anti_join(twitterstop)%>%
    count(word, sort = TRUE)
})

wordfreq_bydate_top20<-lapply(wordfreq_bydate_covid, function(x){
  x %>% top_n(20)
})

#exclude the word coronavirus
wordfreq_bydate_top20_wocoron<-lapply(wordfreq_bydate_covid, function(x){
  x %>% 
    filter(word != "coronavirus")%>%
    top_n(20)
})

#exclude the word coronavirus, and include more words
wordfreq_bydate_top50_wocoron<-lapply(wordfreq_bydate_covid, function(x){
  x %>% 
    filter(!word %in% c("coronavirus", "covid19", "covid", "coronavid19", "corona", "covid2019"))%>%
    top_n(50)
})
```

```{r}
##try plotting one of the date
ds2<-wordfreq_bydate_top20_wocoron[[2]]
#barchart
ds2%>%
  ggplot(aes(word))+geom_bar(aes(weight = n))
#wordcloud
ds2more<-wordfreq_bydate_top50_wocoron[[2]]
wordcloud2(ds2more, size = 1.6, color = 'random-dark')
```


```{r}
# make the document-term matrix.  
#   I sometimes call this the bag-of-words-matrix.
dt = cast_sparse(tt, tweet, word)
dt[1:5,1:10]
str(dt)
dim(dt)
hist(rowSums(dt))
cs = colSums(dt)
hist(log(cs[cs>1]))
```


## Including Plots

You can also embed plots, for example:

```{r pressure, echo=FALSE}
#rtwee tryout
masktweet<-search_tweets(
  "mask", n = 100, include_rts = FALSE
)

coronovirustweet<-search_tweets(
  "coronovirus", n=100, include_rts = FALSE
)

masktweet
coronovirustweet
```

Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.
